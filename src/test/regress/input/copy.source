--
-- COPY
--
-- CLASS POPULATION
--	(any resemblance to real life is purely coincidental)
--
COPY aggtest FROM '@abs_srcdir@/data/agg.data';

COPY onek FROM '@abs_srcdir@/data/onek.data';

COPY onek TO '@abs_builddir@/results/onek.data';

TRUNCATE onek;

COPY onek FROM '@abs_builddir@/results/onek.data';

COPY tenk1 FROM '@abs_srcdir@/data/tenk.data';

COPY slow_emp4000 FROM '@abs_srcdir@/data/rect.data';

COPY person FROM '@abs_srcdir@/data/person.data';

COPY emp FROM '@abs_srcdir@/data/emp.data';

COPY student FROM '@abs_srcdir@/data/student.data';

COPY stud_emp FROM '@abs_srcdir@/data/stud_emp.data';

COPY road FROM '@abs_srcdir@/data/streets.data';

COPY real_city FROM '@abs_srcdir@/data/real_city.data';

COPY hash_i4_heap FROM '@abs_srcdir@/data/hash.data';

COPY hash_name_heap FROM '@abs_srcdir@/data/hash.data';

COPY hash_txt_heap FROM '@abs_srcdir@/data/hash.data';

COPY hash_f8_heap FROM '@abs_srcdir@/data/hash.data';

-- the data in this file has a lot of duplicates in the index key
-- fields, leading to long bucket chains and lots of table expansion.
-- this is therefore a stress test of the bucket overflow code (unlike
-- the data in hash.data, which has unique index keys).
--
-- COPY hash_ovfl_heap FROM '@abs_srcdir@/data/hashovfl.data';

COPY bt_i4_heap FROM '@abs_srcdir@/data/desc.data';

COPY bt_name_heap FROM '@abs_srcdir@/data/hash.data';

COPY bt_txt_heap FROM '@abs_srcdir@/data/desc.data';

COPY bt_f8_heap FROM '@abs_srcdir@/data/hash.data';

COPY array_op_test FROM '@abs_srcdir@/data/array.data';

COPY array_index_op_test FROM '@abs_srcdir@/data/array.data';

--- test copying in CSV mode with various styles
--- of embedded line ending characters

create temp table copytest (
	style	text,
	test 	text,
	filler	int);

insert into copytest values('DOS',E'abc\r\ndef',1);
insert into copytest values('Unix',E'abc\ndef',2);
insert into copytest values('Mac',E'abc\rdef',3);
insert into copytest values(E'esc\\ape',E'a\\r\\\r\\\n\\nb',4);

copy copytest to '@abs_builddir@/results/copytest.csv' csv;

create temp table copytest2 (like copytest);

copy copytest2 from '@abs_builddir@/results/copytest.csv' csv;

select * from copytest except select * from copytest2 order by 1,2,3;

truncate copytest2;

--- same test but with an escape char different from quote char

copy copytest to '@abs_builddir@/results/copytest.csv' csv quote '''' escape E'\\';

copy copytest2 from '@abs_builddir@/results/copytest.csv' csv quote '''' escape E'\\';

select * from copytest except select * from copytest2 order by 1,2,3;


-- test header line feature

create temp table copytest3 (
	c1 int, 
	"col with , comma" text, 
	"col with "" quote"  int) distributed by (c1);

copy copytest3 from stdin csv header;
this is just a line full of junk that would error out if parsed
1,a,1
2,b,2
\.

copy copytest3 to stdout csv header;
-- copy with error table
CREATE TABLE number (a INT) DISTRIBUTED BY (a);

COPY number FROM STDIN LOG ERRORS INTO err_copy SEGMENT REJECT LIMIT 10 ROWS;
these are invalid line should be insert into error table.
a
b
c
d
e
f
g
h
\.

select relname,filename,linenum,bytenum,errmsg,rawdata,rawbytes from err_copy order by linenum;
select * from number; --should be empty
\d err_copy

DROP TABLE err_copy;

COPY number FROM STDIN LOG ERRORS INTO err_copy SEGMENT REJECT LIMIT 10 ROWS;
these are invalid line should be insert into error table.
a
1
b
2
c
3
d
4
e
5
f
6
g
7
h
\.

select relname,filename,linenum,bytenum,errmsg,rawdata,rawbytes from err_copy order by linenum;
select count(*) from number; --should be 7
DROP TABLE err_copy;

TRUNCATE number;

COPY number FROM STDIN LOG ERRORS INTO err_copy SEGMENT REJECT LIMIT 10 ROWS;
these are invalid line should be insert into error table.
a
1
b
2
c
3
d
4
e
5
f
6
g
7
h
i
\.

select relname,filename,linenum,bytenum,errmsg,rawdata,rawbytes from err_copy order by linenum; -- should not exist
select count(*) from number; --should be empty

TRUNCATE number;
CREATE TABLE err_copy (cmdtime timestamp with time zone, relname text, filename text, linenum integer, bytenum integer, errmsg text, rawdata text, rawbytes bytea) distributed randomly;

COPY number FROM STDIN LOG ERRORS INTO err_copy SEGMENT REJECT LIMIT 10 ROWS;
these are invalid line should be insert into error table.
a
1
b
2
c
3
d
4
e
5
f
6
g
7
h
\.

select relname,filename,linenum,bytenum,errmsg,rawdata,rawbytes from err_copy order by linenum;
select count(*) from number; --should be 7
DROP TABLE err_copy;

-- invalid error table schema
TRUNCATE number;
create table invalid_error_table1 (a int) distributed randomly;
create table invalid_error_table3 (cmdtime timestamp with time zone, relname text, filename text, linenum integer, bytenum integer, errmsg text, rawdata text, rawbytes bytea)
		distributed by (cmdtime);
		
COPY number FROM STDIN LOG ERRORS INTO invalid_error_table1 SEGMENT REJECT LIMIT 10 ROWS; -- should fail
these are invalid line should be insert into error table.
1
\.

;

COPY number FROM STDIN LOG ERRORS INTO invalid_error_table3 SEGMENT REJECT LIMIT 10 ROWS; -- should fail
these are invalid line should be insert into error table.
1
\.

;

DROP TABLE invalid_error_table1;
DROP TABLE invalid_error_table3;

DROP TABLE number;
