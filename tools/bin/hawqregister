#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

# Usage1: hawq register [-h hostname] [-p port] [-U username] [-d database] [-f filepath] [-e eof] <tablename>
# Usage2: hawq register [-h hostname] [-p port] [-U username] [-d database] [-c config] [--force] [--repair] <tablename>

import os
import sys
try:
    from gppylib.commands.unix import getLocalHostname, getUserName
    from gppylib.db import dbconn
    from gppylib.gplog import get_default_logger, setup_tool_logging
    from gppylib.gpparseopts import OptParser, OptChecker
    from pygresql import pg
    from hawqpylib.hawqlib import local_ssh, local_ssh_output
except ImportError, e:
    print e
    sys.stderr.write('Cannot import module, please check that you have source greenplum_path.sh\n')
    sys.exit(2)

# setup logging
logger = get_default_logger()
EXECNAME = os.path.split(__file__)[-1]
setup_tool_logging(EXECNAME, getLocalHostname(), getUserName())

def option_parser():
    '''option parser'''
    parser = OptParser(option_class=OptChecker,
                       usage='usage: %prog [options] table_name',
                       version='%prog version $Revision: #1 $')
    parser.remove_option('-h')
    parser.add_option('-?', '--help', action='help')
    parser.add_option('-h', '--host', help='host of the target DB')
    parser.add_option('-p', '--port', help='port of the target DB', type='int', default=0)
    parser.add_option('-U', '--user', help='username of the target DB')
    parser.add_option('-d', '--database', default='postgres', dest='database', help='database name')
    parser.add_option('-f', '--filepath', dest='filepath', help='file name in HDFS')
    parser.add_option('-e', '--eof', dest='filesize', type='int', default=0, help='eof of the file to be registered')
    parser.add_option('-c', '--config', dest='yml_config', default='', help='configuration file in YAML format')
    parser.add_option('-F', '--force', dest='force', action='store_true', default=False)
    parser.add_option('-R', '--repair', dest='repair', action='store_true', default=False)
    return parser


def register_yaml_dict_check(D):
    '''check exists'''
    check_list = ['DFS_URL', 'Distribution_Policy', 'FileFormat', 'TableName', 'Bucketnum']
    for attr in check_list:
        if D.get(attr) is None:
            logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % attr)
            sys.exit(1)
    if D['Bucketnum'] <= 0:
        logger.error('Bucketnum should not be zero, please check your yaml configuration file.')
        sys.exit(1)
    if D['FileFormat'] in ['Parquet', 'AO']:
        prefix = D['FileFormat']
        local_check_list = ['%s_FileLocations' % prefix, '%s_Schema' % prefix]
        for attr in local_check_list:
            if D.get(attr) is None:
                logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % attr)
                sys.exit(1)
        if D['%s_FileLocations' % prefix].get('Files') is None:
            logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % '%s_FileLocations.Files' % prefix)
            sys.exit(1)
        for d in D['%s_FileLocations' % prefix]['Files']:
            if d.get('path') is None:
                logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % '%s_FileLocations.Files.path' % prefix)
                sys.exit(1)
            if d.get('size') is None:
                logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % '%s_FileLocations.Files.size' % prefix)
                sys.exit(1)
    else:
        logger.error('hawq register only support Parquet and AO formats. Format %s is not supported.' % D['FileFormat'])
        sys.exit(1)
    prefix = D['FileFormat']
    if D.get('%s_Schema' % prefix) is None:
        logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % '%s_Schema' % prefix)
        sys.exit(1)
    for d in D['%s_Schema' % prefix]:
        if d.get('name') is None:
            logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % '%s_Schema.name' % prefix)
            sys.exit(1)
        if d.get('type') is None:
            logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % '%s_Schema.type' % prefix)
            sys.exit(1)
    if D['FileFormat'] == 'Parquet':
        sub_check_list = ['CompressionLevel', 'CompressionType', 'PageSize', 'RowGroupSize']
        for attr in sub_check_list:
            if not D['Parquet_FileLocations'].has_key(attr):
                logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % 'Parquet_FileLocations.%s' % attr)
                sys.exit(1)
    else:
        sub_check_list = ['Checksum', 'CompressionLevel', 'CompressionType']
        for attr in sub_check_list:
            if not D['AO_FileLocations'].has_key(attr):
                logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % 'AO_FileLocations.%s' % attr)
                sys.exit(1)

class GpRegisterAccessor(object):
    def __init__(self, conn):
        self.conn = conn
        rows = self.exec_query("""
        SELECT oid, datname, dat2tablespace,
               pg_encoding_to_char(encoding) encoding
        FROM pg_database WHERE datname=current_database()""")
        self.dbid = rows[0]['oid']
        self.dbname = rows[0]['datname']
        self.spcid = rows[0]['dat2tablespace']
        self.dbencoding = rows[0]['encoding']
        self.dbversion = self.exec_query('select version()')[0]['version']

    def exec_query(self, sql):
        '''execute query and return dict result'''
        return self.conn.query(sql).dictresult()

    def get_table_existed(self, tablename):
        qry = """select count(*) from pg_class where relname = '%s';""" % tablename.split('.')[-1].lower()
        return self.exec_query(qry)[0]['count'] == 1

    def do_create_table(self, tablename, schema_info, fmt, distrbution_policy, file_locations, bucket_number, partitionby, partitions_constraint, partitions_name):
        if self.get_table_existed(tablename):
            return False
        schema = ','.join([k['name'] + ' ' + k['type'] for k in schema_info])
        partlist = ""
        for index in range(len(partitions_constraint)):
            if index > 0:
                partlist += ", "
            partlist = partlist + "partition " + partitions_name[index] + " " + partitions_constraint[index]
          
        fmt = 'ROW' if fmt == 'AO' else fmt
        if fmt == 'ROW':
            if partitionby is None:
                query = ('create table %s(%s) with (appendonly=true, orientation=%s, compresstype=%s, compresslevel=%s, checksum=%s, bucketnum=%s) %s;'
                         % (tablename, schema, fmt, file_locations['CompressionType'], file_locations['CompressionLevel'], file_locations['Checksum'], bucket_number, distrbution_policy))
            else:
                query = ('create table %s(%s) with (appendonly=true, orientation=%s, compresstype=%s, compresslevel=%s, checksum=%s, bucketnum=%s) %s %s (%s);'
                         % (tablename, schema, fmt, file_locations['CompressionType'], file_locations['CompressionLevel'], file_locations['Checksum'], bucket_number, distrbution_policy, partitionby, partlist))
        else: # Parquet
            if partitionby is None:
                query = ('create table %s(%s) with (appendonly=true, orientation=%s, compresstype=%s, compresslevel=%s, pagesize=%s, rowgroupsize=%s, bucketnum=%s) %s;'
                         % (tablename, schema, fmt, file_locations['CompressionType'], file_locations['CompressionLevel'], file_locations['PageSize'], file_locations['RowGroupSize'], bucket_number, distrbution_policy))
            else:
                query = ('create table %s(%s) with (appendonly=true, orientation=%s, compresstype=%s, compresslevel=%s, pagesize=%s, rowgroupsize=%s, bucketnum=%s) %s %s (%s);'
                         % (tablename, schema, fmt, file_locations['CompressionType'], file_locations['CompressionLevel'], file_locations['PageSize'], file_locations['RowGroupSize'], bucket_number, distrbution_policy, partitionby, partlist))
        self.conn.query(query)
        return True

    def check_hash_type(self, tablename):
        qry = """select attrnums from gp_distribution_policy, pg_class where pg_class.relname = '%s' and pg_class.oid = gp_distribution_policy.localoid;""" % tablename
        rows = self.exec_query(qry)
        if len(rows) == 0:
            logger.error('Table %s is not an append-only table. There is no record in gp_distribution_policy table.' % tablename)
            sys.exit(1)
        if rows[0]['attrnums']:
            logger.error('Cannot register file(s) to a table which is hash distributed.')
            sys.exit(1)

    # pg_paqseg_#
    def get_seg_name(self, tablename, database, fmt):
        tablename = tablename.split('.')[-1]
        query = ("select pg_class2.relname from pg_class as pg_class1, pg_appendonly, pg_class as pg_class2 "
                 "where pg_class1.relname ='%s' and pg_class1.oid = pg_appendonly.relid and pg_appendonly.segrelid = pg_class2.oid;") % tablename
        rows = self.exec_query(query)
        if len(rows) == 0:
            logger.error('table "%s" not found in db "%s"' % (tablename, database))
            sys.exit(1)
        relname = rows[0]['relname']
        if fmt == 'Parquet':
            if relname.find('paq') == -1:
                logger.error("table '%s' is not parquet format" % tablename)
                sys.exit(1)
        return relname

    def get_distribution_policy_info(self, tablename):
        query = "select oid from pg_class where relname = '%s';" % tablename.split('.')[-1].lower()
        rows = self.exec_query(query)
        oid = rows[0]['oid']
        query = "select * from gp_distribution_policy where localoid = '%s';" % oid
        rows = self.exec_query(query)
        return rows[0]['attrnums']

    def get_partition_info(self, tablename):
        ''' Get partition information from pg_partitions, return a constraint-tablename dictionary '''
        query = "SELECT partitiontablename, partitionboundary FROM pg_partitions WHERE tablename = '%s'" % tablename
        return self.exec_query(query)

    def get_bucket_number(self, tablename):
        query = "select oid from pg_class where relname = '%s';" % tablename.split('.')[-1].lower()
        rows = self.exec_query(query)
        oid = rows[0]['oid']
        query = "select * from gp_distribution_policy where localoid = '%s';" % oid
        rows = self.exec_query(query)
        return rows[0]['bucketnum']

    def get_metadata_from_database(self, tablename, seg_name):
        query = 'select segno from pg_aoseg.%s;' % seg_name
        firstsegno = len(self.exec_query(query)) + 1
        # get the full path of correspoding file for target table
        query = ("select location, gp_persistent_tablespace_node.tablespace_oid, database_oid, relfilenode from pg_class, gp_persistent_relation_node, "
                 "gp_persistent_tablespace_node, gp_persistent_filespace_node where relname = '%s' and pg_class.relfilenode = "
                 "gp_persistent_relation_node.relfilenode_oid and gp_persistent_relation_node.tablespace_oid = gp_persistent_tablespace_node.tablespace_oid "
                 "and gp_persistent_filespace_node.filespace_oid = gp_persistent_filespace_node.filespace_oid;") % tablename.split('.')[-1]
        D = self.exec_query(query)[0]
        tabledir = '/'.join([D['location'].strip(), str(D['tablespace_oid']), str(D['database_oid']), str(D['relfilenode']), ''])
        return firstsegno, tabledir

    def get_database_encoding_indx(self, database):
        query = "select encoding from pg_database where datname = '%s';" % database
        return self.exec_query(query)[0]['encoding']

    def get_database_encoding(self, encoding_indx):
        query = "select pg_encoding_to_char(%s);" % encoding_indx
        return self.exec_query(query)[0]['pg_encoding_to_char']

    def update_catalog(self, query):
        self.conn.query(query)


class HawqRegister(object):
    def __init__(self, options, table, utility_conn, conn):
        self.yml = options.yml_config
        self.filepath = options.filepath
        self.database = options.database
        self.tablename = table
        self.filesize = options.filesize
        self.accessor = GpRegisterAccessor(conn)
        self.utility_accessor = GpRegisterAccessor(utility_conn)
        self.mode = self._init_mode(options.force, options.repair)
        self._init()

    def _init_mode(self, force, repair):
        def table_existed():
            return self.accessor.get_table_existed(self.tablename)

        if self.yml:
            if force:
                return 'force'
            elif repair:
                if not table_existed():
                    logger.error('--repair mode asserts the table has been already created.')
                    sys.exit(1)
                return 'repair'
            else:
                return 'second_normal'
        else:
            return 'first'

    def _init(self):
        def check_hash_type():
            self.accessor.check_hash_type(self.tablename)

        # check conflicting distributed policy
        def check_distribution_policy():
            if self.distribution_policy.startswith('DISTRIBUTED BY'):
                if len(self.files) % self.bucket_number != 0:
                    logger.error('Files to be registered must be multiple times to the bucket number of hash table.')
                    sys.exit(1)

        def check_database_encoding():
            encoding_indx = self.accessor.get_database_encoding_indx(self.database)
            encoding = self.accessor.get_database_encoding(encoding_indx)
            if self.encoding.strip() != encoding:
                logger.error('Database encoding from yaml configuration file(%s) is not consistent with encoding from input args(%s).' % (self.encoding, encoding))
                sys.exit(1)

        def create_table():
            return self.accessor.do_create_table(self.tablename, self.schema, self.file_format, self.distribution_policy, self.file_locations, self.bucket_number,
                                                 self.partitionby, self.partitions_constraint, self.partitions_name)

        def get_seg_name():
            return self.utility_accessor.get_seg_name(self.tablename, self.database, self.file_format)

        def get_metadata():
            return self.accessor.get_metadata_from_database(self.tablename, self.seg_name)

        def get_distribution_policy():
            return self.accessor.get_distribution_policy_info(self.tablename)

        def check_policy_consistency():
            policy = get_distribution_policy() # "" or "{1,3}"
            if policy is None:
                if ' '.join(self.distribution_policy.strip().split()).lower() == 'distributed randomly':
                    return
                else:
                    logger.error('Distribution policy of %s from yaml file is not consistent with the policy of existing table.' % self.tablename)
                    sys.exit(1)
            tmp_dict = {}
            for i, d in enumerate(self.schema):
                tmp_dict[d['name']] = i + 1
            # 'DISTRIBUETD BY (1,3)' -> {1,3}
            cols = self.distribution_policy.strip().split()[-1].strip('(').strip(')').split(',')
            original_policy = ','.join([str(tmp_dict[col]) for col in cols])
            if policy.strip('{').strip('}') != original_policy:
                logger.error('Distribution policy of %s from yaml file is not consistent with the policy of existing table.' % self.tablename)
                sys.exit(1)

        def check_bucket_number():
            def get_bucket_number():
                return self.accessor.get_bucket_number(self.tablename)

            if self.bucket_number != get_bucket_number():
                logger.error('Bucket number of %s is not consistent with previous bucket number.' % self.tablename)
                sys.exit(1)
        
        def set_yml_dataa(file_format, files, sizes, schema, distribution_policy, file_locations,\
                          bucket_number, partitionby, partitions_constraint, partitions_name, partitions_compression_level,\
                          partitions_compression_type, partitions_checksum, partitions_filepaths, partitions_filesizes, encoding):
            self.file_format = file_format
            self.files = files
            self.sizes = sizes
            self.schema = schema
            self.distribution_policy = distribution_policy
            self.file_locations = file_locations
            self.bucket_number = bucket_number
            self.partitionby = partitionby
            self.partitions_constraint = partitions_constraint
            self.partitions_name = partitions_name 
            self.partitions_compression_level = partitions_compression_level
            self.partitions_compression_type = partitions_compression_type
            self.partitions_checksum = partitions_checksum
            self.partitions_filepaths = partitions_filepaths 
            self.partitions_filesizes = partitions_filesizes
            self.encoding = encoding

        def option_parser_yml(yml_file):
            import yaml
            with open(yml_file, 'r') as f:
                params = yaml.load(f)
            register_yaml_dict_check(params)
            partitions_filepaths = []
            partitions_filesizes = []
            partitions_constraint = []
            partitions_name = []
            partitions_checksum = []
            partitions_compression_level = []
            partitions_compression_type = []
            files, sizes = [], []
            
            if params['FileFormat'].lower() == 'parquet':
                partitionby = params.get('Parquet_FileLocations').get('PartitionBy')
                if params.get('Parquet_FileLocations').get('Partitions') and len(params['Parquet_FileLocations']['Partitions']):
                    partitions_checksum = [d['Checksum'] for d in params['Parquet_FileLocations']['Partitions']]
                    partitions_compression_level = [d['CompressionLevel'] for d in params['Parquet_FileLocations']['Partitions']]
                    partitions_compression_type = [d['CompressionType'] for d in params['Parquet_FileLocations']['Partitions']]
                    partitions_constraint = [d['Constraint'] for d in params['Parquet_FileLocations']['Partitions']]
                    partitions_files = [d['Files'] for d in params['Parquet_FileLocations']['Partitions']]
                    if len(partitions_files):
                        for pfile in partitions_files:
                            partitions_filepaths.append([params['DFS_URL'] + item['path'] for item in pfile])
                            partitions_filesizes.append([item['size'] for item in pfile])
                    partitions_name = [d['Name'] for d in params['Parquet_FileLocations']['Partitions']]
                if len(params['Parquet_FileLocations']['Files']):
                    files, sizes = [params['DFS_URL'] + d['path'] for d in params['Parquet_FileLocations']['Files']], [d['size'] for d in params['Parquet_FileLocations']['Files']]
                encoding = params['Encoding']
                set_yml_dataa('Parquet', files, sizes, params['Parquet_Schema'], params['Distribution_Policy'], params['Parquet_FileLocations'], params['Bucketnum'], partitionby,\
                              partitions_constraint, partitions_name, partitions_compression_level, partitions_compression_type, partitions_checksum, partitions_filepaths, partitions_filesizes, encoding)
                
            else: #AO format
                partitionby = params.get('AO_FileLocations').get('PartitionBy')
                if params.get('AO_FileLocations').get('Partitions') and len(params['AO_FileLocations']['Partitions']):
                    partitions_checksum = [d['Checksum'] for d in params['AO_FileLocations']['Partitions']]
                    partitions_compressionLevel = [d['CompressionLevel'] for d in params['AO_FileLocations']['Partitions']]
                    partitions_compressionType = [d['CompressionType'] for d in params['AO_FileLocations']['Partitions']]
                    partitions_constraint = [d['Constraint'] for d in params['AO_FileLocations']['Partitions']]
                    partitions_files = [d['Files'] for d in params['AO_FileLocations']['Partitions']]
                    if len(partitions_files):
                        for pfile in partitions_files:
                            partitions_filepaths.append([params['DFS_URL'] + item['path'] for item in pfile])
                            partitions_filesizes.append([item['size'] for item in pfile])
                    partitions_name = [d['Name'] for d in params['AO_FileLocations']['Partitions']]
                if len(params['AO_FileLocations']['Files']):
                    files, sizes = [params['DFS_URL'] + d['path'] for d in params['AO_FileLocations']['Files']], [d['size'] for d in params['AO_FileLocations']['Files']]
                encoding = params['Encoding']
                set_yml_dataa('AO', files, sizes, params['AO_Schema'], params['Distribution_Policy'], params['AO_FileLocations'], params['Bucketnum'], partitionby, partitions_constraint,\
                              partitions_name, partitions_compression_level, partitions_compression_type, partitions_checksum, partitions_filepaths, partitions_filesizes, encoding)
                
        def check_file_not_folder():
            for fn in self.files:
                hdfscmd = 'hdfs dfs -test -f %s' % fn
                if local_ssh(hdfscmd, logger):
                    logger.info('%s is not a file in hdfs, please check the yaml configuration file.' % fn)
                    sys.exit(1)

        def check_sizes_valid():
            for k, fn in enumerate(self.files):
                hdfscmd = 'hdfs dfs -du %s' % fn
                _, out, _ = local_ssh_output(hdfscmd)
                if self.sizes[k] > int(out.strip().split()[0]):
                    logger.error('File size(%s) in yaml configuration file should not exceed actual length(%s) of file %s.' % (self.sizes[k], out.strip().split()[0], fn))
                    sys.exit(1)

        if self.yml:
            option_parser_yml(options.yml_config)
            self.filepath = self.files[0][:self.files[0].rfind('/')] if self.files else ''
            check_file_not_folder()
            check_database_encoding()
            if self.mode != 'force' and self.mode != 'repair':
                if not create_table():
                    self.mode = 'second_exist'
            check_bucket_number()
            check_distribution_policy()
            check_policy_consistency()
        else:
            self.file_format = 'Parquet'
            check_hash_type() # Usage1 only support randomly distributed table
        if not self.filepath:
            if self.mode == 'first':
                logger.info('Please specify filepath with -f option.')
            else:
                logger.info('Hawq Register Succeed.')
            sys.exit(0)

        self.seg_name = get_seg_name()
        self.firstsegno, self.tabledir = get_metadata()

        if self.mode == 'repair':
            if self.tabledir.strip('/') != self.filepath.strip('/'):
                logger.error("In repair mode, file path from yaml file should be the same with table's path.")
                sys.exit(1)
            existed_files, existed_sizes = self._get_files_in_hdfs(self.filepath)
            existed_info = {}
            for k, fn in enumerate(existed_files):
                existed_info[fn] = existed_sizes[k]
            for k, fn in enumerate(self.files):
                if fn not in existed_files:
                    logger.error('Can not register in repair mode since giving non-existing file: %s.' % fn)
                    sys.exit(1)
                if self.sizes[k] > existed_info[fn]:
                    logger.error('Can not register in repair mode since giving larger file size: %s' % self.sizes[k])
                    sys.exit(1)

        if self.mode == 'second_exist':
            if self.tabledir.strip('/') == self.filepath.strip('/'):
                logger.error('Files to be registered should not be the same with table path.')
                sys.exit(1)

        self.do_not_move, self.files_update, self.sizes_update = False, [], []
        if self.mode == 'force':
            existed_files, _ = self._get_files_in_hdfs(self.tabledir)
            if len(self.files) == len(existed_files):
                if sorted(self.files) != sorted(existed_files):
                    logger.error('In force mode, you should include existing table files in yaml configuration file. Otherwise you should drop the previous table before register --force.')
                    sys.exit(1)
                else:
                    self.do_not_move, self.files_update, self.sizes_update = True, self.files, self.sizes
                    self.files, self.sizes = [], []
            else:
                files_old, sizes_old = [f for f in self.files], [sz for sz in self.sizes]
                for k, f in enumerate(files_old):
                    if f in existed_files:
                        self.files_update.append(files_old[k])
                        self.sizes_update.append(sizes_old[k])
                        self.files.remove(files_old[k])
                        self.sizes.remove(sizes_old[k])
        elif self.mode == 'repair':
            self.do_not_move = True
            self.files_update, self.sizes_update = [fn for fn in self.files], [sz for sz in self.sizes]
            self.files_delete = []
            for fn in existed_files:
                if fn not in self.files:
                    self.files_delete.append(fn)
            self.files, self.sizes = [], []

        self._check_files_and_table_in_same_hdfs_cluster(self.filepath, self.tabledir)

        if not self.yml:
            self.files, self.sizes = self._get_files_in_hdfs(self.filepath)
        print 'New file(s) to be registered: ', self.files
        if self.files_update:
            print 'Catalog info need to be updated for these files: ', self.files_update

        if self.filesize:
            if len(self.files) != 1:
                logger.error('-e option is only supported with single file case.')
                sys.exit(1)
            self.sizes = [self.filesize]
        check_sizes_valid()

        if self.file_format == 'Parquet':
            self._check_parquet_format(self.files)

    def _get_partition_info(self):
        dic = {}
        for ele in self.accessor.get_partition_info(self.tablename):
            dic[ele['partitionboundary']] = ele['partitiontablename']
        return dic

    def _check_files_and_table_in_same_hdfs_cluster(self, filepath, tabledir):
        '''Check whether all the files refered by 'filepath' and the location corresponding to the table are in the same hdfs cluster'''
        if not filepath:
            return
        # check whether the files to be registered is in hdfs
        filesystem = filepath.split('://')
        if filesystem[0] != 'hdfs':
            logger.error('Only support registering file(s) in hdfs.')
            sys.exit(1)
        fileroot = filepath.split('/')
        tableroot = tabledir.split('/')
        # check the root url of them. eg: for 'hdfs://localhost:8020/temp/tempfile', we check 'hdfs://localohst:8020'
        if fileroot[0:3] != tableroot[0:3]:
            logger.error("Files to be registered and the table are not in the same hdfs cluster.\nFile(s) to be registered: '%s'\nTable path in HDFS: '%s'." % (filepath, tabledir))
            sys.exit(1)

    def _get_files_in_hdfs(self, filepath):
        '''Get all the files refered by 'filepath', which could be a file or a directory containing all the files'''
        files, sizes = [], []
        hdfscmd = "hdfs dfs -test -e %s" % filepath
        result = local_ssh(hdfscmd, logger)
        if result != 0:
            logger.error("Path '%s' does not exist in hdfs" % filepath)
            sys.exit(1)
        hdfscmd = "hdfs dfs -ls -R %s" % filepath
        result, out, err = local_ssh_output(hdfscmd)
        outlines = out.splitlines()
        # recursively search all the files under path 'filepath'
        for line in outlines:
            lineargs = line.split()
            if len(lineargs) == 8 and lineargs[0].find("d") == -1:
                files.append(lineargs[7])
                sizes.append(int(lineargs[4]))
        if len(files) == 0:
            logger.error("Dir '%s' is empty" % filepath)
            sys.exit(1)
        return files, sizes

    def _check_parquet_format(self, files):
        '''Check whether the file to be registered is parquet format'''
        for f in files:
            hdfscmd = 'hdfs dfs -du -h %s | head -c 1' % f
            rc, out, err = local_ssh_output(hdfscmd)
            if out == '0':
                continue
            hdfscmd = 'hdfs dfs -cat %s | head -c 4 | grep PAR1' % f
            result1 = local_ssh(hdfscmd, logger)
            hdfscmd = 'hdfs dfs -cat %s | tail -c 4 | grep PAR1' % f
            result2 = local_ssh(hdfscmd, logger)
            if result1 or result2:
                logger.error('File %s is not parquet format' % f)
                sys.exit(1)

    def _move_files_in_hdfs(self):
        '''Move file(s) in src path into the folder correspoding to the target table'''
        segno = self.firstsegno
        for f in self.files:
            srcfile = f
            dstfile = self.tabledir + str(segno)
            segno += 1
            if srcfile != dstfile:
                hdfscmd = 'hdfs dfs -mv %s %s' % (srcfile, dstfile)
                sys.stdout.write('hdfscmd: "%s"\n' % hdfscmd)
                result = local_ssh(hdfscmd, logger)
                if result != 0:
                    logger.error('Fail to move %s to %s' % (srcfile, dstfile))
                    sys.exit(1)

    def _delete_files_in_hdfs(self):
        for fn in self.files_delete:
            hdfscmd = 'hdfs dfs -rm %s' % fn
            sys.stdout.write('hdfscmd: "%s"\n' % hdfscmd)
            result = local_ssh(hdfscmd, logger)
            if result != 0:
                logger.error('Fail to delete %s ' % fn)
                sys.exit(1)

    def _modify_metadata(self, mode):
        if mode == 'insert':
            eofs = self.sizes
            query = "set allow_system_table_mods='dml';"
            if self.file_format == 'Parquet':
                query += 'insert into pg_aoseg.%s values(%d, %d, %d, %d)' % (self.seg_name, self.firstsegno, eofs[0], -1, -1)
                for k, eof in enumerate(eofs[1:]):
                    query += ',(%d, %d, %d, %d)' % (self.firstsegno + k + 1, eof, -1, -1)
            else:
                query += 'insert into pg_aoseg.%s values(%d, %d, %d, %d, %d)' % (self.seg_name, self.firstsegno, eofs[0], -1, -1, -1)
                for k, eof in enumerate(eofs[1:]):
                    query += ',(%d, %d, %d, %d, %d)' % (self.firstsegno + k + 1, eof, -1, -1, -1)
            query += ';'
        elif mode == 'update':
            eofs = self.sizes_update
            query = "set allow_system_table_mods='dml';"
            query += "begin transaction;"
            segno_lst = [f.split('/')[-1] for f in self.files_update]
            if self.file_format == 'Parquet':
                for i, eof in enumerate(eofs):
                    query += "update pg_aoseg.%s set eof = '%s', tupcount = '%s', eofuncompressed = '%s' where segno = '%s';" % (self.seg_name, eof, -1, -1, segno_lst[i])
            else:
                for i, eof in enumerate(eofs):
                    query += "update pg_aoseg.%s set eof = '%s', tupcount = '%s', varblockcount = '%s', eofuncompressed = '%s' where segno = '%s';" % (self.seg_name, eof, -1, -1, -1, segno_lst[i])
            query += "end transaction;"
        else: # update_and_insert
            eofs = self.sizes
            query = "set allow_system_table_mods='dml';"
            query += "begin transaction;"
            if self.file_format == 'Parquet':
                query += 'insert into pg_aoseg.%s values(%d, %d, %d, %d)' % (self.seg_name, self.firstsegno, eofs[0], -1, -1)
                for k, eof in enumerate(eofs[1:]):
                    query += ',(%d, %d, %d, %d)' % (self.firstsegno + k + 1, eof, -1, -1)
            else:
                query += 'insert into pg_aoseg.%s values(%d, %d, %d, %d, %d)' % (self.seg_name, self.firstsegno, eofs[0], -1, -1, -1)
                for k, eof in enumerate(eofs[1:]):
                    query += ',(%d, %d, %d, %d, %d)' % (self.firstsegno + k + 1, eof, -1, -1, -1)
            query += ';'

            segno_lst = [f.split('/')[-1] for f in self.files_update]
            if self.file_format == 'Parquet':
                for i, eof in enumerate(self.sizes_update):
                    query += "update pg_aoseg.%s set eof = '%s', tupcount = '%s', eofuncompressed = '%s' where segno = '%s';" % (self.seg_name, eof, -1, -1, segno_lst[i])
            else:
                for i, eof in enumerate(self.sizes_update):
                    query += "update pg_aoseg.%s set eof = '%s', tupcount = '%s', varblockcount = '%s', eofuncompressed = '%s' where segno = '%s';" % (self.seg_name, eof, -1, -1, -1, segno_lst[i])
            query += "end transaction;"
        return self.utility_accessor.update_catalog(query)

    def _delete_metadata(self):
        query = "set allow_system_table_mods='dml';"
        query += "begin transaction;"
        segno_lst = [fn.strip().split('/')[-1] for fn in self.files_delete]
        for seg in segno_lst:
            query += "delete from pg_aoseg.%s where segno = '%s';" % (self.seg_name, seg)
        query += "end transaction;"
        return self.utility_accessor.update_catalog(query)

    def _mapping_tablename_from_yml(self, partitions):
        ''' Mapping table name from yml file, return a list of (table_name,(file_path, file_size)) '''
        mappings = []
        for pos, constraint in enumerate(self.partitions_constraint):
            if partitions.has_key(constraint):
                mappings.extend([(partitions[constraint], (self.partitions_filepaths[pos][i], self.partitions_filesizes[pos][i]))
                                for i in xrange(len(self.partitions_filepaths[pos]))])
        return mappings

    def register(self):
        if not self.do_not_move:
            self._move_files_in_hdfs()
        if (not self.do_not_move) and self.mode == 'force':
            self._modify_metadata('update_and_insert')
        else:
            if self.mode == 'force':
                self._modify_metadata('update')
            elif self.mode == 'repair':
                self._modify_metadata('update')
                if self.files_delete:
                    self._delete_files_in_hdfs()
                    self._delete_metadata()
            else:
                self._modify_metadata('insert')
        logger.info('Hawq Register Succeed.')


def main(options, args):
    def connectdb(options):
        '''
        Trying to connect database, return a connection object.
        If failed to connect, raise a pg.InternalError
        '''
        url = dbconn.DbURL(hostname=options.host, port=options.port,
                           dbname=options.database, username=options.user)
        logger.info('try to connect database %s:%s %s' % (url.pghost, url.pgport, url.pgdb))
        utility_conn = pg.connect(dbname=url.pgdb, host=url.pghost, port=url.pgport,
                                  user=url.pguser, passwd=url.pgpass, opt='-c gp_session_role=utility')
        conn = pg.connect(dbname=url.pgdb, host=url.pghost, port=url.pgport,
                          user=url.pguser, passwd=url.pgpass)
        return utility_conn, conn

    # connect db
    try:
        utility_conn, conn = connectdb(options)
    except pg.InternalError:
        logger.error('Fail to connect to database, this script can only be run when database is up.')
        return 1
    # register
    ins = HawqRegister(options, args[0], utility_conn, conn)
    ins.register()
    conn.close()


if __name__ == '__main__':
    parser = option_parser()
    options, args = parser.parse_args()
    if len(args) != 1 or (options.force and options.repair):
        parser.print_help(sys.stderr)
        sys.exit(1)
    if (options.yml_config or options.force or options.repair) and options.filepath:
        parser.print_help(sys.stderr)
        sys.exit(1)
    if local_ssh('hdfs'):
        logger.error('Command "hdfs" is not available.')
        sys.exit(1)
    main(options, args)
