#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

# Usage1: hawq register [-h hostname] [-p port] [-U username] [-d database] [-f filepath] tablename
# Usage2: hawq register [-h hostname] [-p port] [-U username] [-d database] [-c config] tablename
import os, sys, optparse, getpass, re, urlparse
try:
    from gppylib.commands.unix import getLocalHostname, getUserName
    from gppylib.db import dbconn
    from gppylib.gplog import get_default_logger, setup_tool_logging
    from gppylib.gpparseopts import OptParser, OptChecker
    from pygresql import pg
    from pygresql.pgdb import DatabaseError
    from hawqpylib.hawqlib import local_ssh, local_ssh_output
except ImportError, e:
    print e
    sys.stderr.write('cannot import module, please check that you have source greenplum_path.sh\n')
    sys.exit(2)

# setup logging
logger = get_default_logger()
EXECNAME = os.path.split(__file__)[-1]
setup_tool_logging(EXECNAME,getLocalHostname(),getUserName())


def option_parser():
    parser = OptParser(option_class=OptChecker,
                       usage='usage: %prog [options] table_name',
                       version='%prog version $Revision: #1 $')
    parser.remove_option('-h')
    parser.add_option('-?', '--help', action='help')
    parser.add_option('-h', '--host', help='host of the target DB')
    parser.add_option('-p', '--port', help='port of the target DB', type='int', default=0)
    parser.add_option('-U', '--user', help='username of the target DB')
    parser.add_option('-d', '--database', default = 'postgres', dest = 'database', help='database name')
    parser.add_option('-f', '--filepath', dest = 'filepath', help='file name in HDFS')
    parser.add_option('-c', '--config', dest = 'yml_config', default = '', help='configuration file in YAML format')
    return parser


def register_yaml_dict_check(D):
    # check exists
    check_list = ['DFS_URL', 'Distribution_Policy', 'FileFormat', 'TableName']
    for attr in check_list:
        if not D.get(attr):
            logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % attr)
            sys.exit(1)
    if D['FileFormat'] in ['Parquet', 'AO']:
        prefix = D['FileFormat']
        local_check_list = ['%s_FileLocations' % prefix, '%s_Schema' % prefix]
        for attr in local_check_list:
            if not D.get(attr):
                logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % attr)
                sys.exit(1)
        if not D['%s_FileLocations' % prefix].get('Files'):
            logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % '%s_FileLocations.Files' % prefix)
            sys.exit(1)
        for d in D['%s_FileLocations' % prefix]['Files']:
            if not d.get('path'):
                logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % '%s_FileLocations.Files.path' % prefix)
                sys.exit(1)
            if not d.get('size'):
                logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % '%s_FileLocations.Files.size' % prefix)
                sys.exit(1)
    else:
        logger.error('hawq register only support Parquet and AO formats. Format %s is not supported.' % D['FileFormat'])
        sys.exit(1)
    prefix = D['FileFormat']
    if not D.get('%s_Schema' % prefix):
        logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % '%s_Schema' % prefix)
        sys.exit(1)
    for d in D['%s_Schema' % prefix]:
        if not d.get('name'):
            logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % '%s_Schema.name' % prefix)
            sys.exit(1)
        if not d.get('type'):
            logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % '%s_Schema.type' % prefix)
            sys.exit(1)


def option_parser_yml(yml_file):
    import yaml
    with open(yml_file, 'r') as f:
        params = yaml.load(f)
    register_yaml_dict_check(params)
    if params['FileFormat'] == 'Parquet':
        if not len(params['Parquet_FileLocations']['Files']):
            return 'Parquet', '', params['Parquet_Schema'], params['Distribution_Policy']
        offset = params['Parquet_FileLocations']['Files'][0]['path'].rfind('/')
        filepath = (params['DFS_URL'] + params['Parquet_FileLocations']['Files'][0]['path'][:offset]
                    if len(params['Parquet_FileLocations']['Files']) != 1
                    else params['DFS_URL'] + params['Parquet_FileLocations']['Files'][0]['path'])
        return 'Parquet', filepath, params['Parquet_Schema'], params['Distribution_Policy']
    if not len(params['AO_FileLocations']['Files']):
        return 'AO', '', params['AO_Schema'], params['Distribution_Policy']
    offset = params['AO_FileLocations']['Files'][0]['path'].rfind('/')
    filepath = (params['DFS_URL'] + params['AO_FileLocations']['Files'][0]['path'][:offset]
                if len(params['AO_FileLocations']['Files']) != 1
                else params['DFS_URL'] + params['AO_FileLocations']['Files'][0]['path'])
    return 'AO', filepath, params['AO_Schema'], params['Distribution_Policy']


def create_table(dburl, tablename, schema_info, fmt, distrbution_policy):
    try:
        schema = ','.join([k['name'] + ' ' + k['type'] for k in schema_info])
        fmt = 'ROW' if fmt == 'AO' else fmt
        query = 'create table %s(%s) with (appendonly=true, orientation=%s) %s;' % (tablename, schema, fmt, distrbution_policy)
        conn = dbconn.connect(dburl, False)
        rows = dbconn.execSQL(conn, query)
        conn.commit()
    except DatabaseError, ex:
        logger.error('Failed to execute query ""%s"' % query)
        sys.exit(1)


def get_seg_name(dburl, tablename, database, fmt):
    try:
        relname = ''
        tablename = tablename.split('.')[-1]
        query = ("select pg_class2.relname from pg_class as pg_class1, pg_appendonly, pg_class as pg_class2 "
                 "where pg_class1.relname ='%s' and pg_class1.oid = pg_appendonly.relid and pg_appendonly.segrelid = pg_class2.oid;") % tablename
        conn = dbconn.connect(dburl, True)
        rows = dbconn.execSQL(conn, query)
        conn.commit()
        if not rows.rowcount:
            logger.error('table "%s" not found in db "%s"' % (tablename, database))
            sys.exit(1)
        for row in rows:
            relname = row[0]
        conn.close()
    except DatabaseError, ex:
        logger.error('Failed to run query "%s" with dbname "%s"' % (query, database))
        sys.exit(1)
    if fmt == 'Parquet':
        if relname.find("paq") == -1:
            logger.error("table '%s' is not parquet format" % tablename)
            sys.exit(1)

    return relname


def check_hash_type(dburl, tablename):
    '''Check whether target table is hash distributed, in that case simple insertion does not work'''
    try:
        query = "select attrnums from gp_distribution_policy, pg_class where pg_class.relname = '%s' and pg_class.oid = gp_distribution_policy.localoid;" % tablename
        conn = dbconn.connect(dburl, False)
        rows = dbconn.execSQL(conn, query)
        conn.commit()
        if not rows.rowcount:
            logger.error('Table not found in table gp_distribution_policy.' % tablename)
            sys.exit(1)
        for row in rows:
            if row[0]:
                logger.error('Cannot register file(s) to a table which is hash distribuetd.')
                sys.exit(1)
        conn.close()
    except DatabaseError, ex:
        logger.error('Failed to execute query "%s"' % query)
        sys.exit(1)


def get_metadata_from_database(dburl, tablename, seg_name):
    '''Get the metadata to be inserted from hdfs'''
    try:
        query = 'select segno from pg_aoseg.%s;' % seg_name
        conn = dbconn.connect(dburl, False)
        rows = dbconn.execSQL(conn, query)
        conn.commit()
        conn.close()
    except DatabaseError, ex:
        logger.error('Failed to execute query "%s"' % query)
        sys.exit(1)

    firstsegno = rows.rowcount + 1

    try:
        # get the full path of correspoding file for target table
        query = ("select location, gp_persistent_tablespace_node.tablespace_oid, database_oid, relfilenode from pg_class, gp_persistent_relation_node, "
             "gp_persistent_tablespace_node, gp_persistent_filespace_node where relname = '%s' and pg_class.relfilenode = "
             "gp_persistent_relation_node.relfilenode_oid and gp_persistent_relation_node.tablespace_oid = gp_persistent_tablespace_node.tablespace_oid "
             "and gp_persistent_filespace_node.filespace_oid = gp_persistent_filespace_node.filespace_oid;") % tablename.split('.')[-1]
        conn = dbconn.connect(dburl, False)
        rows = dbconn.execSQL(conn, query)
        conn.commit()
        conn.close()
    except DatabaseError, ex:
        logger.error('Failed to execute query "%s"' % query)
        sys.exit(1)
    for row in rows:
        tabledir = '/'.join([row[0].strip(), str(row[1]), str(row[2]), str(row[3]), ''])
    return firstsegno, tabledir


def check_files_and_table_in_same_hdfs_cluster(filepath, tabledir):
    '''Check whether all the files refered by 'filepath' and the location corresponding to the table are in the same hdfs cluster'''
    # check whether the files to be registered is in hdfs
    filesystem = filepath.split('://')
    if filesystem[0] != 'hdfs':
        logger.error('Only support to register file(s) in hdfs')
        sys.exit(1)
    fileroot = filepath.split('/')
    tableroot = tabledir.split('/')
    # check the root url of them. eg: for 'hdfs://localhost:8020/temp/tempfile', we check 'hdfs://localohst:8020'
    if fileroot[0:3] != tableroot[0:3]:
        logger.error("Files to be registered and the table are not in the same hdfs cluster.\nFile(s) to be registered: '%s'\nTable path in HDFS: '%s'" % (filepath, tabledir))
        sys.exit(1)


def get_files_in_hdfs(filepath):
    '''Get all the files refered by 'filepath', which could be a file or a directory containing all the files'''
    files = []
    sizes = []
    hdfscmd = "hadoop fs -test -e %s" % filepath
    result = local_ssh(hdfscmd)
    if result != 0:
        logger.error("Path '%s' does not exist in hdfs" % filepath)
        sys.exit(1)
    hdfscmd = "hadoop fs -ls -R %s" % filepath
    result, out, err = local_ssh_output(hdfscmd)
    outlines = out.splitlines()
    # recursively search all the files under path 'filepath'
    for line in outlines:
        lineargs = line.split()
        if len(lineargs) == 8 and lineargs[0].find ("d") == -1:
            files.append(lineargs[7])
            sizes.append(int(lineargs[4]))
    if len(files) == 0:
        logger.error("Dir '%s' is empty" % filepath)
        sys.exit(1)
    return files, sizes


def check_parquet_format(files):
    '''Check whether the file to be registered is parquet format'''
    for f in files:
        hdfscmd = 'hadoop fs -du -h %s | head -c 1' % f
        rc, out, err = local_ssh_output(hdfscmd)
        if out == '0':
            continue
        hdfscmd = 'hadoop fs -cat %s | head -c 4 | grep PAR1' % f
        result1 = local_ssh(hdfscmd)
        hdfscmd = 'hadoop fs -cat %s | tail -c 4 | grep PAR1' % f
        result2 = local_ssh(hdfscmd)
        if result1 or result2:
            logger.error('File %s is not parquet format' % f)
            sys.exit(1)


def move_files_in_hdfs(databasename, tablename, files, firstsegno, tabledir, normal):
    '''Move file(s) in src path into the folder correspoding to the target table'''
    if normal:
        segno = firstsegno
        for file in files:
            srcfile = file
            dstfile = tabledir + str(segno)
            segno += 1
            if srcfile != dstfile:
                hdfscmd = 'hadoop fs -mv %s %s' % (srcfile, dstfile)
                sys.stdout.write('hdfscmd: "%s"\n' % hdfscmd)
                result = local_ssh(hdfscmd)
                if result != 0:
                    logger.error('Fail to move %s to %s' % (srcfile, dstfile))
                    sys.exit(1)
    else:
        segno = firstsegno
        for file in files:
            dstfile = file
            srcfile = tabledir + str(segno)
            segno += 1
            if srcfile != dstfile:
                hdfscmd = 'hadoop fs -mv %s %s' % (srcfile, dstfile)
                sys.stdout.write('hdfscmd: "%s"\n' % hdfscmd)
                result = local_ssh(hdfscmd)
                if result != 0:
                    logger.error('Fail to move "%s" to "%s"' % (srcfile, dstfile))
                    sys.exit(1)


def insert_metadata_into_database(dburl, databasename, tablename, seg_name, firstsegno, tabledir, eofs):
    '''Insert the metadata into database'''
    try:
        query = "SET allow_system_table_mods='dml';"
        segno = firstsegno
        for eof in eofs:
            query += "insert into pg_aoseg.%s values(%d, %d, %d, %d);" % (seg_name, segno, eof, -1, -1)
            segno += 1
        conn = dbconn.connect(dburl, True)
        rows = dbconn.execSQL(conn, query)
        conn.commit()
        conn.close()
    except DatabaseError, ex:
        logger.error('Failed to connect to database, this script can only be run when the database is up')
        move_files_in_hdfs(options.database, options.tablename, files, firstsegno, tabledir, False)
        sys.exit(1)


if __name__ == '__main__':
    parser = option_parser()
    options, args = parser.parse_args()
    if len(args) != 1 or (options.yml_config and options.filepath):
        parser.print_help(sys.stderr)
        sys.exit(1)
    if local_ssh('hadoop'):
        logger.error('command "hadoop" is not available.')
        sys.exit(1)

    dburl = dbconn.DbURL(hostname=options.host, port=options.port, username=options.user, dbname=options.database)
    filepath, database, tablename = options.filepath, options.database, args[0]

    if options.yml_config: # Usage2
        fileformat, filepath, schema, distribution_policy = option_parser_yml(options.yml_config)
        create_table(dburl, tablename, schema, fileformat, distribution_policy)
    else:
        fileformat = 'Parquet'
        check_hash_type(dburl, tablename) # Usage1 only support randomly distributed table

    seg_name = get_seg_name(dburl, tablename, database, fileformat)
    firstsegno, tabledir = get_metadata_from_database(dburl, tablename, seg_name)
    check_files_and_table_in_same_hdfs_cluster(filepath, tabledir)

    files, sizes = get_files_in_hdfs(filepath)
    print 'File(s) to be registered:', files
    if fileformat == 'Parquet':
        check_parquet_format(files)
    print files
    move_files_in_hdfs(database, tablename, files, firstsegno, tabledir, True)
    insert_metadata_into_database(dburl, database, tablename, seg_name, firstsegno, tabledir, sizes)
    logger.info('Hawq Register Succeed.')
