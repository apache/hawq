#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

# Usage1: hawq register [-h hostname] [-p port] [-U username] [-d database] [-f filepath] [-e eof] <tablename>
# Usage2: hawq register [-h hostname] [-p port] [-U username] [-d database] [-c config] [--force] [--repair] <tablename>

import os
import sys
try:
    from gppylib.commands.unix import getLocalHostname, getUserName
    from gppylib.db import dbconn
    from gppylib.gplog import get_default_logger, setup_tool_logging
    from gppylib.gpparseopts import OptParser, OptChecker
    from pygresql import pg
    from hawqpylib.hawqlib import local_ssh, local_ssh_output
except ImportError, e:
    print e
    sys.stderr.write('Cannot import module, please check that you have source greenplum_path.sh\n')
    sys.exit(2)

# setup logging
logger = get_default_logger()
EXECNAME = os.path.split(__file__)[-1]
setup_tool_logging(EXECNAME, getLocalHostname(), getUserName())

def option_parser():
    '''option parser'''
    parser = OptParser(option_class=OptChecker,
                       usage='usage: %prog [options] table_name',
                       version='%prog version $Revision: #1 $')
    parser.remove_option('-h')
    parser.add_option('-?', '--help', action='help')
    parser.add_option('-h', '--host', help='host of the target DB')
    parser.add_option('-p', '--port', help='port of the target DB', type='int', default=0)
    parser.add_option('-U', '--user', help='username of the target DB')
    parser.add_option('-d', '--database', default='postgres', dest='database', help='database name')
    parser.add_option('-f', '--filepath', dest='filepath', help='file name in HDFS')
    parser.add_option('-e', '--eof', dest='filesize', type='int', default=None, help='eof of the file to be registered')
    parser.add_option('-c', '--config', dest='yml_config', default='', help='configuration file in YAML format')
    parser.add_option('-F', '--force', dest='force', action='store_true', default=False)
    parser.add_option('-R', '--repair', dest='repair', action='store_true', default=False)
    return parser


def register_yaml_dict_check(D, table_column_num, src_tablename):
    '''check exists'''
    check_list = ['DFS_URL', 'Distribution_Policy', 'FileFormat', 'TableName']
    for attr in check_list:
        if D.get(attr) is None:
            logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % attr)
            sys.exit(1)
    if D['Distribution_Policy'].startswith('DISTRIBUTED BY'):
        if D.get('Bucketnum') is None:
            logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % attr)
            sys.exit(1)
        if D['Bucketnum'] <= 0:
            logger.error('Bucketnum should not be zero, please check your yaml configuration file.')
            sys.exit(1)
    if D['FileFormat'] in ['Parquet', 'AO']:
        prefix = D['FileFormat']
        local_check_list = ['%s_FileLocations' % prefix, '%s_Schema' % prefix]
        for attr in local_check_list:
            if D.get(attr) is None:
                logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % attr)
                sys.exit(1)
        if D['%s_FileLocations' % prefix].get('Files') is None:
            logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % '%s_FileLocations.Files' % prefix)
            sys.exit(1)
        for d in D['%s_FileLocations' % prefix]['Files']:
            if d.get('path') is None:
                logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % '%s_FileLocations.Files.path' % prefix)
                sys.exit(1)
            if d.get('size') is None:
                logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % '%s_FileLocations.Files.size' % prefix)
                sys.exit(1)
    else:
        logger.error('hawq register only support Parquet and AO formats. Format %s is not supported.' % D['FileFormat'])
        sys.exit(1)
    prefix = D['FileFormat']
    if D.get('%s_Schema' % prefix) is None:
        logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % '%s_Schema' % prefix)
        sys.exit(1)
    for d in D['%s_Schema' % prefix]:
        if d.get('name') is None:
            logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % '%s_Schema.name' % prefix)
            sys.exit(1)
        if d.get('type') is None:
            logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % '%s_Schema.type' % prefix)
            sys.exit(1)
    if D['FileFormat'] == 'Parquet':
        sub_check_list = ['CompressionLevel', 'CompressionType', 'PageSize', 'RowGroupSize']
        for attr in sub_check_list:
            if not D['Parquet_FileLocations'].has_key(attr):
                logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % 'Parquet_FileLocations.%s' % attr)
                sys.exit(1)
    else:
        sub_check_list = ['Checksum', 'CompressionLevel', 'CompressionType']
        for attr in sub_check_list:
            if not D['AO_FileLocations'].has_key(attr):
                logger.error('Wrong configuration yaml file format: "%s" attribute does not exist.\n See example in "hawq register --help".' % 'AO_FileLocations.%s' % attr)
                sys.exit(1)
    if D['FileFormat'].lower() == 'parquet':
        yml_column_num = len(D['Parquet_Schema'])
    else:
        yml_column_num = len(D['AO_Schema'])
    if table_column_num != yml_column_num and table_column_num > 0:
        logger.error('Column number of table in yaml file is not equals to the column number of table %s.' % src_tablename)
        sys.exit(1)


class FailureHandler(object):
    def __init__(self, conn):
        self.operations = []
        self.conn = conn

    def commit(self, cmd):
        self.operations.append(cmd)

    def assemble_SQL(self, cmd):
        return 'DROP TABLE %s' % cmd[cmd.find('table')+6:cmd.find('(')]

    def assemble_hdfscmd(self, cmd):
        lst = cmd.strip().split()
        return ' '.join(lst[:-2] + [lst[-1], lst[-2]])

    def rollback(self):
        if len(self.operations) != 0:
            logger.info('Error found, Hawqregister starts to rollback...')
        for (typ, cmd) in reversed(self.operations):
            if typ == 'SQL':
                sql = self.assemble_SQL(cmd)
                try:
                    self.conn.query(sql)
                except pg.DatabaseError as e:
                    logger.error('Rollback failure: %s.' % sql)
                    print e
                    sys.exit(1)
            if typ == 'HDFSCMD':
                hdfscmd = self.assemble_hdfscmd(cmd)
                sys.stdout.write('Rollback hdfscmd: "%s"\n' % hdfscmd)
                result = local_ssh(hdfscmd, logger)
                if result != 0:
                    logger.error('Fail to rollback: %s.' % hdfscmd)
                    sys.exit(1)
        if len(self.operations) != 0:
            logger.info('Hawq Register Rollback Finished.')


class GpRegisterAccessor(object):
    def __init__(self, conn):
        self.conn = conn
        rows = self.exec_query("""
        SELECT oid, datname, dat2tablespace,
               pg_encoding_to_char(encoding) encoding
        FROM pg_database WHERE datname=current_database()""")
        self.dbid = rows[0]['oid']
        self.dbname = rows[0]['datname']
        self.spcid = rows[0]['dat2tablespace']
        self.dbencoding = rows[0]['encoding']
        self.dbversion = self.exec_query('select version()')[0]['version']

    def exec_query(self, sql):
        '''execute query and return dict result'''
        return self.conn.query(sql).dictresult()

    def get_table_existed(self, tablename):
        qry = """select count(*) from pg_class where relname = '%s';""" % tablename.split('.')[-1].lower()
        return self.exec_query(qry)[0]['count'] == 1

    def get_table_column_num(self, tablename):
        qry = """select count(*) from pg_attribute ,pg_class where pg_class.relname = '%s' and pg_class.oid = pg_attribute.attrelid and attnum > 0;""" % tablename.split('.')[-1].lower()
        return self.exec_query(qry)[0]['count']

    def do_create_table(self, src_table_name, tablename, schema_info, fmt, distrbution_policy, file_locations, bucket_number, partitionby, partitions_constraint, partitions_name):
        if self.get_table_existed(tablename):
            return False, ''
        schema = ','.join([k['name'] + ' ' + k['type'] for k in schema_info])
        partlist = ""
        for index in range(len(partitions_constraint)):
            if index > 0:
                partlist += ", "
            partition_refine_name = partitions_name[index]
            splitter = src_table_name.split(".")[-1] + '_1_prt_'
            partition_refine_name = partition_refine_name.split(splitter)[-1]
            #in some case, constraint contains "partition XXX" but in other case, it doesn't contain. we need to treat them separately.
            if partitions_constraint[index].strip().startswith("DEFAULT PARTITION") or partitions_constraint[index].strip().startswith("PARTITION") or (len(partition_refine_name) > 0 and partition_refine_name[0].isdigit()):
                partlist = partlist + " " + partitions_constraint[index]
            else:
                partlist = partlist + "PARTITION " + partition_refine_name + " " + partitions_constraint[index]

        fmt = 'ROW' if fmt == 'AO' else fmt
        if fmt == 'ROW':
            if partitionby is None:
                query = ('create table %s(%s) with (appendonly=true, orientation=%s, compresstype=%s, compresslevel=%s, checksum=%s, bucketnum=%s) %s;'
                         % (tablename, schema, fmt, file_locations['CompressionType'], file_locations['CompressionLevel'], file_locations['Checksum'], bucket_number, distrbution_policy))
            else:
                query = ('create table %s(%s) with (appendonly=true, orientation=%s, compresstype=%s, compresslevel=%s, checksum=%s, bucketnum=%s) %s %s (%s);'
                         % (tablename, schema, fmt, file_locations['CompressionType'], file_locations['CompressionLevel'], file_locations['Checksum'], bucket_number, distrbution_policy, partitionby, partlist))
        else: # Parquet
            if partitionby is None:
                query = ('create table %s(%s) with (appendonly=true, orientation=%s, compresstype=%s, compresslevel=%s, pagesize=%s, rowgroupsize=%s, bucketnum=%s) %s;'
                         % (tablename, schema, fmt, file_locations['CompressionType'], file_locations['CompressionLevel'], file_locations['PageSize'], file_locations['RowGroupSize'], bucket_number, distrbution_policy))
            else:
                query = ('create table %s(%s) with (appendonly=true, orientation=%s, compresstype=%s, compresslevel=%s, pagesize=%s, rowgroupsize=%s, bucketnum=%s) %s %s (%s);'
                         % (tablename, schema, fmt, file_locations['CompressionType'], file_locations['CompressionLevel'], file_locations['PageSize'], file_locations['RowGroupSize'], bucket_number, distrbution_policy, partitionby, partlist))
        self.conn.query(query)
        return True, query

    def check_hash_type(self, tablename):
        qry = """select attrnums from gp_distribution_policy, pg_class where pg_class.relname = '%s' and pg_class.oid = gp_distribution_policy.localoid;""" % tablename
        rows = self.exec_query(qry)
        if len(rows) == 0:
            logger.error('Table %s is not an append-only table. There is no record in gp_distribution_policy table.' % tablename)
            sys.exit(1)
        if rows[0]['attrnums']:
            logger.error('Cannot register file(s) to a table which is hash distributed.')
            sys.exit(1)

    # pg_paqseg_#
    def get_seg_name(self, tablename, database, fmt):
        tablename = tablename.split('.')[-1]
        query = ("select pg_class2.relname from pg_class as pg_class1, pg_appendonly, pg_class as pg_class2 "
                 "where pg_class1.relname ='%s' and pg_class1.oid = pg_appendonly.relid and pg_appendonly.segrelid = pg_class2.oid;") % tablename
        rows = self.exec_query(query)
        if len(rows) == 0:
            logger.error('table "%s" not found in db "%s"' % (tablename, database))
            return ('', False)
        relname = rows[0]['relname']
        if fmt == 'Parquet':
            if relname.find('paq') == -1:
                logger.error("table '%s' is not parquet format" % tablename)
                return ('', False)
        return (relname, True)

    def get_distribution_policy_info(self, tablename):
        query = "select oid from pg_class where relname = '%s';" % tablename.split('.')[-1].lower()
        rows = self.exec_query(query)
        oid = rows[0]['oid']
        query = "select * from gp_distribution_policy where localoid = '%s';" % oid
        rows = self.exec_query(query)
        return rows[0]['attrnums']

    def get_bucket_number(self, tablename):
        query = "select oid from pg_class where relname = '%s';" % tablename.split('.')[-1].lower()
        rows = self.exec_query(query)
        oid = rows[0]['oid']
        query = "select * from gp_distribution_policy where localoid = '%s';" % oid
        rows = self.exec_query(query)
        return rows[0]['bucketnum']

    def get_metadata_from_database(self, tablename, seg_name):
        query = 'select segno from pg_aoseg.%s;' % seg_name
        firstsegno = len(self.exec_query(query)) + 1
        # get the full path of correspoding file for target table
        query = ("select location, gp_persistent_tablespace_node.tablespace_oid, database_oid, relfilenode from pg_class, gp_persistent_relation_node, "
                 "gp_persistent_tablespace_node, gp_persistent_filespace_node where relname = '%s' and pg_class.relfilenode = "
                 "gp_persistent_relation_node.relfilenode_oid and gp_persistent_relation_node.tablespace_oid = gp_persistent_tablespace_node.tablespace_oid "
                 "and gp_persistent_filespace_node.filespace_oid = gp_persistent_filespace_node.filespace_oid;") % tablename.split('.')[-1]
        D = self.exec_query(query)[0]
        tabledir = '/'.join([D['location'].strip(), str(D['tablespace_oid']), str(D['database_oid']), str(D['relfilenode']), ''])
        return firstsegno, tabledir

    def get_metadata_from_seg_name(self, seg_name):
        query = 'select segno, eof from pg_aoseg.%s;' % seg_name
        rows = self.exec_query(query)
        return [str(row['segno']) for row in rows], [int(row['eof']) for row in rows]

    def get_database_encoding_indx(self, database):
        query = "select encoding from pg_database where datname = '%s';" % database
        return self.exec_query(query)[0]['encoding']

    def get_database_encoding(self, encoding_indx):
        query = "select pg_encoding_to_char(%s);" % encoding_indx
        return self.exec_query(query)[0]['pg_encoding_to_char']

    def update_catalog(self, query):
        self.conn.query(query)


class HawqRegister(object):
    def __init__(self, options, table, utility_conn, conn, failure_handler):
        self.yml = options.yml_config
        self.filepath = options.filepath
        self.database = options.database
        self.dst_table_name = table.lower()
        self.tablename = table.lower()
        self.filesize = options.filesize
        self.accessor = GpRegisterAccessor(conn)
        self.utility_accessor = GpRegisterAccessor(utility_conn)
        self.failure_handler = failure_handler
        self.mode = self._init_mode(options.force, options.repair)
        self.srcfiles = []
        self.dstfiles = []
        self._init()

    def _init_mode(self, force, repair):
        def table_existed():
            return self.accessor.get_table_existed(self.dst_table_name)

        if self.yml:
            if force:
                return 'force'
            elif repair:
                logger.info('--repair mode is not supported in current release of hawq register.')
                sys.exit(0)
                if not table_existed():
                    logger.error('--repair mode asserts the table has been already created.')
                    sys.exit(1)
                return 'repair'
            else:
                return 'usage2_table_not_exist'
        else:
            if not table_existed():
                logger.error('Table %s does not exist.\nYou should create table before registering the data.' % self.dst_table_name)
                sys.exit(1)
            else:
                return 'usage1'

    def _check_hash_type(self):
        self.accessor.check_hash_type(self.dst_table_name)

    def _create_table(self):
        try:
           (ret, query) = self.accessor.do_create_table(self.src_table_name, self.dst_table_name, self.schema, self.file_format, self.distribution_policy, self.file_locations, self.bucket_number,
                                                             self.partitionby, self.partitions_constraint, self.partitions_name)
        except pg.DatabaseError as e:
            print e
            sys.exit(1)
        if ret:
            self.failure_handler.commit(('SQL', query))
        return ret

    def _check_database_encoding(self):
        encoding_indx = self.accessor.get_database_encoding_indx(self.database)
        encoding = self.accessor.get_database_encoding(encoding_indx)
        if self.encoding.strip() != encoding:
            logger.error('Database encoding from yaml configuration file(%s) is not consistent with encoding from input args(%s).' % (self.encoding, encoding))
            sys.exit(1)

    def _check_policy_consistency(self):
        policy = self._get_distribution_policy() # "" or "{1,3}"
        if policy is None:
            return
        if self.distribution_policy == 'DISTRIBUTED RANDOMLY':
            logger.error('Distribution policy of %s from yaml is not consistent with the policy of existing table.' % self.tablename)
            self.failure_handler.rollback()
            sys.exit(1)
        tmp_dict = {}
        for i, d in enumerate(self.schema):
            tmp_dict[d['name']] = i + 1
        # 'DISTRIBUETD BY (1,3)' -> {1,3}
        cols = self.distribution_policy.strip().split()[-1].strip('(').strip(')').split(',')
        original_policy = ','.join([str(tmp_dict[col]) for col in cols])
        if policy.strip('{').strip('}') != original_policy:
            logger.error('Distribution policy of %s from yaml file is not consistent with the policy of existing table.' % self.dst_table_name)
            self.failure_handler.rollback()
            sys.exit(1)

    def _set_yml_dataa(self, file_format, files, sizes, tablename, schema, distribution_policy, file_locations,\
                      bucket_number, partitionby, partitions_constraint, partitions_name, partitions_compression_level,\
                      partitions_compression_type, partitions_checksum, partitions_filepaths, partitions_filesizes, encoding):
        self.file_format = file_format
        self.files = files
        self.sizes = sizes
        self.src_table_name = tablename
        self.schema = schema
        self.distribution_policy = distribution_policy
        self.file_locations = file_locations
        self.bucket_number = bucket_number
        self.partitionby = partitionby
        self.partitions_constraint = partitions_constraint
        self.partitions_name = partitions_name
        self.partitions_compression_level = partitions_compression_level
        self.partitions_compression_type = partitions_compression_type
        self.partitions_checksum = partitions_checksum
        self.partitions_filepaths = partitions_filepaths
        self.partitions_filesizes = partitions_filesizes
        self.encoding = encoding
        self.files_same_path = []
        self.sizes_same_path = []
        self.segnos_same_path = []

    def _option_parser_yml(self, yml_file):
        import yaml
        try:
            with open(yml_file, 'r') as f:
                params = yaml.load(f)
        except yaml.scanner.ScannerError as e:
            print e
            self.failure_handler.rollback()
            sys.exit(1)
        table_column_num = self.accessor.get_table_column_num(self.tablename)
        register_yaml_dict_check(params, table_column_num, self.tablename)
        partitions_filepaths = []
        partitions_filesizes = []
        partitions_constraint = []
        partitions_name = []
        partitions_checksum = []
        partitions_compression_level = []
        partitions_compression_type = []
        files, sizes = [], []

        if params['FileFormat'].lower() == 'parquet':
            Format = 'Parquet'
        else: #AO format
            Format = 'AO'
        Format_FileLocations = '%s_FileLocations' % Format
        partitionby = params.get(Format_FileLocations).get('PartitionBy')
        if partitionby:
            logger.info('Partition table is not supported in current release of hawq register.')
            sys.exit(0)
        if params.get(Format_FileLocations).get('Partitions') and len(params[Format_FileLocations]['Partitions']):
            partitions_checksum = [d['Checksum'] for d in params[Format_FileLocations]['Partitions']]
            partitions_compression_level = [d['CompressionLevel'] for d in params[Format_FileLocations]['Partitions']]
            partitions_compression_type = [d['CompressionType'] for d in params[Format_FileLocations]['Partitions']]
            partitions_constraint = [d['Constraint'] for d in params[Format_FileLocations]['Partitions']]
            partitions_files = [d['Files'] for d in params[Format_FileLocations]['Partitions']]
            if len(partitions_files):
                for pfile in partitions_files:
                    partitions_filepaths.append([params['DFS_URL'] + item['path'] for item in pfile])
                    partitions_filesizes.append([item['size'] for item in pfile])
            partitions_name = [d['Name'] for d in params[Format_FileLocations]['Partitions']]
        if len(params[Format_FileLocations]['Files']):
            files, sizes = [params['DFS_URL'] + d['path'] for d in params[Format_FileLocations]['Files']], [d['size'] for d in params[Format_FileLocations]['Files']]
        encoding = params['Encoding']
        bucketNum = params['Bucketnum'] if params['Distribution_Policy'].startswith('DISTRIBUTED BY') else 6
        self._set_yml_dataa(Format, files, sizes, params['TableName'], params['%s_Schema' % Format], params['Distribution_Policy'], params[Format_FileLocations], bucketNum, partitionby,\
                      partitions_constraint, partitions_name, partitions_compression_level, partitions_compression_type, partitions_checksum, partitions_filepaths, partitions_filesizes, encoding)


    # check conflicting distributed policy
    def _check_distribution_policy(self):
        if self.distribution_policy.startswith('DISTRIBUTED BY'):
            if len(self.files) % self.bucket_number != 0:
                logger.error('Files to be registered must be multiple times to the bucket number of hash table.')
                self.failure_handler.rollback()
                sys.exit(1)

    def _get_seg_name(self):
        return self.utility_accessor.get_seg_name(self.tablename, self.database, self.file_format)

    def _get_metadata(self):
        return self.accessor.get_metadata_from_database(self.tablename, self.seg_name)

    def _get_metadata_from_table(self):
        return self.accessor.get_metadata_from_seg_name(self.seg_name)

    def _get_distribution_policy(self):
        return self.accessor.get_distribution_policy_info(self.tablename)

    def _check_bucket_number(self):
        def get_bucket_number():
            return self.accessor.get_bucket_number(self.tablename)

        if self.bucket_number != get_bucket_number():
            logger.error('Bucket number of %s is not consistent with previous bucket number.' % self.tablename)
            self.failure_handler.rollback()
            sys.exit(1)

    def _check_file_not_folder(self):
        for fn in self.files:
            hdfscmd = 'hadoop fs -test -f %s' % fn
            if local_ssh(hdfscmd, logger):
                logger.info('%s is not a file in hdfs, please check the yaml configuration file.' % fn)
                sys.exit(1)

    def _is_folder(self, filepath):
        hdfscmd = 'hadoop fs -test -d %s' % filepath
        if local_ssh(hdfscmd, logger):
            return False
        else:
            return True

    def _check_sizes_valid(self):
        for sz in self.sizes:
            if type(sz) != type(1):
                logger.error('File size(%s) in yaml configuration file should be int type.' % sz)
                self.failure_handler.rollback()
                sys.exit(1)
            if sz < 0:
                logger.error('File size(%s) in yaml configuration file should not be less than 0.' % sz)
                self.failure_handler.rollback()
                sys.exit(1)
        hdfscmd = 'hadoop fs -du %s' % ' '.join(self.files)
        _, outs, _ = local_ssh_output(hdfscmd)
        outs = outs.split('\n')
        for k, out in enumerate(outs):
            if self.sizes[k] > int(out.strip().split()[0]):
                if self.mode == 'usage1':
                    logger.error('Specified file size(%s) should not exceed actual length(%s) of file %s.' % (self.sizes[k], out.strip().split()[0], self.files[k]))
                else:
                    logger.error('File size(%s) in yaml configuration file should not exceed actual length(%s) of file %s.' % (self.sizes[k], out.strip().split()[0], self.files[k]))
                self.failure_handler.rollback()
                sys.exit(1)

    def _check_no_regex_filepath(self, files):
        for fn in files:
            tmp_lst = fn.split('/')
            for v in tmp_lst:
                if v == '.':
                    logger.error('Hawq register does not support file path with regex: %s.' % fn)
                    self.failure_handler.rollback()
                    sys.exit(1)
            for ch in ['..', '*']:
                if fn.find(ch) != -1:
                    logger.error('Hawq register does not support file path with regex: %s.' % fn)
                    self.failure_handler.rollback()
                    sys.exit(1)

    def _init(self):
        if self.yml:
            self._option_parser_yml(options.yml_config)
            self.filepath = self.files[0][:self.files[0].rfind('/')] if self.files else ''
            self._check_file_not_folder()
            self._check_database_encoding()
            if self.mode != 'repair':
                if not self._create_table() and self.mode != 'force':
                    self.mode = 'usage2_table_exist'
        else:
            if self._is_folder(self.filepath) and self.filesize:
                logger.error('-e option is only supported with single file case.')
                sys.exit(1)
            self.file_format = 'Parquet'
            self.files_same_path = []
            self.sizes_same_path = []
            self.segnos_same_path = []
            self._check_hash_type() # Usage1 only support randomly distributed table
        self.queries = "set allow_system_table_mods='dml';"
        self.queries += "begin transaction;"
        self._do_check()
        self._prepare_register()
        self.queries += "end transaction;"

    def _do_check(self):
        if self.yml:
            self._check_bucket_number()
            self._check_distribution_policy()
            self._check_policy_consistency()
            self._check_no_regex_filepath(self.files)
        if not self.filepath:
            if self.mode == 'usage1':
                logger.info('Please specify filepath with -f option.')
            else:
                logger.info('Hawq Register Succeed.')
            sys.exit(0)

        (self.seg_name, tmp_ret) = self._get_seg_name()
        if not tmp_ret:
            self.failure_handler.rollback()
            sys.exit(1)
        self.firstsegno, self.tabledir = self._get_metadata()
        
        seg_list, existed_sizes = self._get_metadata_from_table()
        existed_files = [self.tabledir + seg for seg in seg_list]
        # check if file numbers in hdfs is consistent with the record count of pg_aoseg.
        hdfs_file_no_lst = [f.split('/')[-1] for f in existed_files]
        for k in range(1, self.firstsegno - 1):
            if self.firstsegno - 1 > len(existed_files) or str(k) not in hdfs_file_no_lst:
                logger.error("Hawq aoseg metadata doesn't consistent with file numbers in hdfs.")
                self.failure_handler.rollback()
                sys.exit(1)
        if self.mode == 'repair':
            if self.tabledir.strip('/') != self.filepath.strip('/'):
                logger.error("In repair mode, file path from yaml file should be the same with table's path.")
                self.failure_handler.rollback()
                sys.exit(1)
            existed_info = {}
            for k, fn in enumerate(existed_files):
                existed_info[fn] = existed_sizes[k]
            for k, fn in enumerate(self.files):
                if fn not in existed_files:
                    logger.error('Can not register in repair mode since giving non-existing file: %s.' % fn)
                    self.failure_handler.rollback()
                    sys.exit(1)
                if self.sizes[k] > existed_info[fn]:
                    logger.error('Can not register in repair mode since giving larger file size: %s' % self.sizes[k])
                    self.failure_handler.rollback()
                    sys.exit(1)

        if self.mode == 'usage2_table_exist':
            if self.tabledir.strip('/') == self.filepath.strip('/'):
                logger.error('Files to be registered should not be the same with table path.')
                self.failure_handler.rollback()
                sys.exit(1)

        if not self.yml:
            self._check_no_regex_filepath([self.filepath])
            self.files, self.sizes = self._get_files_in_hdfs(self.filepath)

        self.do_not_move, self.files_update, self.sizes_update = False, [], []
        self.files_append, self.sizes_append = [f for f in self.files], [sz for sz in self.sizes]
        if self.mode == 'force':
            if len(self.files) == len(existed_files):
                if sorted(self.files) != sorted(existed_files):
                    logger.error('In force mode, you should include existing table files in yaml configuration file. Otherwise you should drop the previous table before register --force.')
                    self.failure_handler.rollback()
                    sys.exit(1)
                else:
                    self.do_not_move, self.files_update, self.sizes_update = True, self.files, self.sizes
                self.files_append, self.sizes_append = [],[]
            elif len(self.files) < len(existed_files):
                logger.error('In force mode, you should include existing table files in yaml configuration file. Otherwise you should drop the previous table before register --force.')
                self.failure_handler.rollback()
                sys.exit(1)
            else:
                for k, f in enumerate(self.files):
                    if f in existed_files:
                        self.files_update.append(self.files[k])
                        self.sizes_update.append(self.sizes[k])
                        self.files_append.remove(self.files[k])
                        self.sizes_append.remove(self.sizes[k])
                if sorted(self.files_update) != sorted(existed_files):
                    logger.error('In force mode, you should include existing table files in yaml configuration file. Otherwise you should drop the previous table before register --force.')
                    self.failure_handler.rollback()
                    sys.exit(1)
        elif self.mode == 'repair':
            self.do_not_move = True
            self.files_update, self.sizes_update = [fn for fn in self.files], [sz for sz in self.sizes]
            self.files_delete = []
            for fn in existed_files:
                if fn not in self.files:
                    self.files_delete.append(fn)
            self.files_append, self.sizes_append = [], []

        self._check_files_and_table_in_same_hdfs_cluster(self.filepath, self.tabledir)

        print 'New file(s) to be registered: ', self.files_append
        if self.files_update:
            print 'Catalog info need to be updated for these files: ', self.files_update

        if self.filesize is not None:
            if len(self.files) != 1:
                logger.error('-e option is only supported with single file case.')
                self.failure_handler.rollback()
                sys.exit(1)
            self.sizes_append = [self.filesize]
            self.sizes = [self.filesize]
        self._check_sizes_valid()

        if self.file_format == 'Parquet':
            self._check_parquet_format(self.files)

    def _check_files_and_table_in_same_hdfs_cluster(self, filepath, tabledir):
        '''Check whether all the files refered by 'filepath' and the location corresponding to the table are in the same hdfs cluster'''
        if not filepath:
            return
        # check whether the files to be registered is in hdfs
        filesystem = filepath.split('://')
        if filesystem[0] != 'hdfs':
            logger.error('Only support registering file(s) in hdfs.')
            self.failure_handler.rollback()
            sys.exit(1)
        fileroot = filepath.split('/')
        tableroot = tabledir.split('/')
        # check the root url of them. eg: for 'hdfs://localhost:8020/temp/tempfile', we check 'hdfs://localohst:8020'
        if fileroot[0:3] != tableroot[0:3]:
            logger.error("Files to be registered and the table are not in the same hdfs cluster.\nFile(s) to be registered: '%s'\nTable path in HDFS: '%s'." % (filepath, tabledir))
            self.failure_handler.rollback()
            sys.exit(1)

    def _get_files_in_hdfs(self, filepath):
        '''Get all the files refered by 'filepath', which could be a file or a directory containing all the files'''
        files, sizes = [], []
        hdfscmd = "hadoop fs -test -e %s" % filepath
        result = local_ssh(hdfscmd, logger)
        if result != 0:
            logger.error("Path '%s' does not exist in hdfs" % filepath)
            self.failure_handler.rollback()
            sys.exit(1)
        hdfscmd = "hadoop fs -ls -R %s" % filepath
        result, out, err = local_ssh_output(hdfscmd)
        outlines = out.splitlines()
        # recursively search all the files under path 'filepath'
        for line in outlines:
            lineargs = line.split()
            if len(lineargs) == 8 and lineargs[0].find("d") == -1:
                files.append(lineargs[7])
                sizes.append(int(lineargs[4]))
        if len(files) == 0 and self.mode == 'usage1':
            logger.info('No files to be registered.')
            logger.info('Hawq Register Succeed.')
            sys.exit(0)
        if len(files) == 0 and self.mode != 'force':
            logger.error("Dir '%s' is empty" % filepath)
            self.failure_handler.rollback()
            sys.exit(1)
        return files, sizes

    def _check_parquet_format(self, files):
        '''Check whether the file to be registered is parquet format'''
        for f in files:
            hdfscmd = 'hadoop fs -du -h %s | head -c 1' % f
            rc, out, err = local_ssh_output(hdfscmd)
            if out == '0':
                continue
            hdfscmd = 'hadoop fs -cat %s | head -c 4 | grep PAR1' % f
            result1 = local_ssh(hdfscmd)
            hdfscmd = 'hadoop fs -cat %s | tail -c 4 | grep PAR1' % f
            result2 = local_ssh(hdfscmd)
            if result1 or result2:
                logger.error('File %s is not parquet format' % f)
                self.failure_handler.rollback()
                sys.exit(1)

    def _set_move_files_in_hdfs(self):
        segno = self.firstsegno
        # set self.files_same_path, self.sizes_same_path and self.segnos_same_path, which are for files existed in HDFS but not in catalog metadata
        update_segno_lst = [f.split('/')[-1] for f in self.files_update]
        catalog_lst = [str(i) for i in range(1, segno)]
        new_catalog_lst = [str(i) for i in range(segno, len(self.files_update) + 1)]
        exist_catalog_lst = []
        for k, seg in enumerate(update_segno_lst):
            if seg not in catalog_lst:
                self.files_same_path.append(self.files_update[k])
                self.sizes_same_path.append(self.sizes_update[k])
            if seg in new_catalog_lst:
                exist_catalog_lst.append(seg)
        for seg in update_segno_lst:
            if seg not in catalog_lst:
                if seg in exist_catalog_lst:
                    self.segnos_same_path.append(int(seg))
                else:
                    while (str(segno) in exist_catalog_lst):
                        segno += 1
                    self.segnos_same_path.append(segno)

        for k, f in enumerate(self.files_same_path):
            self.srcfiles.append(f)
            self.dstfiles.append(self.tabledir + str(self.segnos_same_path[k]))

        for f in self.files_append:
            self.srcfiles.append(f)
            self.dstfiles.append(self.tabledir + str(segno))
            segno += 1

    def _move_files_in_hdfs(self):
        '''Move file(s) in src path into the folder correspoding to the target table'''
        for k, srcfile in enumerate(self.srcfiles):
            dstfile = self.dstfiles[k]
            if srcfile != dstfile:
                hdfscmd = 'hadoop fs -mv %s %s' % (srcfile, dstfile)
                sys.stdout.write('hdfscmd: "%s"\n' % hdfscmd)
                result = local_ssh(hdfscmd, logger)
                if result != 0:
                    logger.error('Fail to move %s to %s' % (srcfile, dstfile))
                    self.failure_handler.rollback()
                    sys.exit(1)
                self.failure_handler.commit(('HDFSCMD', hdfscmd))

    def _delete_files_in_hdfs(self):
        for fn in self.files_delete:
            hdfscmd = 'hadoop fs -rm %s' % fn
            sys.stdout.write('hdfscmd: "%s"\n' % hdfscmd)
            result = local_ssh(hdfscmd, logger)
            if result != 0:
                logger.error('Fail to delete %s ' % fn)
                self.failure_handler.rollback()
                sys.exit(1)

    def _set_modify_metadata(self, mode):
        segno = self.firstsegno
        append_eofs = self.sizes_append
        update_eofs = self.sizes_update
        same_path_eofs = self.sizes_same_path
        update_segno_lst = [f.split('/')[-1] for f in self.files_update]
        same_path_segno_lst = [seg for seg in self.segnos_same_path]
        query = ""
        if mode == 'force':
            query += "delete from pg_aoseg.%s;" % (self.seg_name)
        
        if self.file_format == 'Parquet': 
            if len(update_eofs) > 0:
                query += 'insert into pg_aoseg.%s values(%s, %d, %d, %d)' % (self.seg_name, update_segno_lst[0], update_eofs[0], -1, -1)
                for k, update_eof in enumerate(update_eofs[1:]):
                    query += ',(%s, %d, %d, %d)' % (update_segno_lst[k + 1], update_eof, -1, -1)
                query += ';'
            if len(same_path_eofs) > 0:
                query += 'insert into pg_aoseg.%s values(%d, %d, %d, %d)' % (self.seg_name, same_path_segno_lst[0], same_path_eofs[0], -1, -1)
                for k, same_path_eof in enumerate(same_path_eofs[1:]):
                    query += ',(%d, %d, %d, %d)' % (same_path_segno_lst[k + 1], same_path_eof, -1, -1)
                query += ';'
            segno += len(same_path_eofs)
            if len(append_eofs) > 0:
                query += 'insert into pg_aoseg.%s values(%d, %d, %d, %d)' % (self.seg_name, segno, append_eofs[0], -1, -1)
                for k, append_eof in enumerate(append_eofs[1:]):
                    query += ',(%d, %d, %d, %d)' % (segno + k + 1, append_eof, -1, -1)
                query += ';'
        else:
            if len(update_eofs) > 0:
                query += 'insert into pg_aoseg.%s values(%s, %d, %d, %d, %d)' % (self.seg_name, update_segno_lst[0], update_eofs[0], -1, -1, -1)
                for k, update_eof in enumerate(update_eofs[1:]):
                    query += ',(%s, %d, %d, %d, %d)' % (update_segno_lst[k + 1], update_eof, -1, -1, -1)
                query += ';'
            if len(same_path_eofs) > 0:
                query += 'insert into pg_aoseg.%s values(%d, %d, %d, %d, %d)' % (self.seg_name, same_path_segno_lst[0], same_path_eofs[0], -1, -1, -1)
                for k, same_path_eof in enumerate(same_path_eofs[1:]):
                    query += ',(%d, %d, %d, %d, %d)' % (same_path_segno_lst[k + 1], same_path_eof, -1, -1, -1)
                query += ';'
            segno += len(same_path_eofs)
            if len(append_eofs) > 0:
                query += 'insert into pg_aoseg.%s values(%d, %d, %d, %d, %d)' % (self.seg_name, segno, append_eofs[0], -1, -1, -1)
                for k, append_eof in enumerate(append_eofs[1:]):
                    query += ',(%d, %d, %d, %d, %d)' % (segno + k + 1, append_eof, -1, -1, -1)
                query += ';'
        self.queries += query
    
    def _modify_metadata(self):
        try:
            self.utility_accessor.update_catalog(self.queries)
        except pg.DatabaseError as e:
            print e
            self.failure_handler.rollback()
            sys.exit(1)

    def _delete_metadata(self):
        query = "set allow_system_table_mods='dml';"
        query += "begin transaction;"
        segno_lst = [fn.strip().split('/')[-1] for fn in self.files_delete]
        for seg in segno_lst:
            query += "delete from pg_aoseg.%s where segno = '%s';" % (self.seg_name, seg)
        query += "end transaction;"
        return self.utility_accessor.update_catalog(query)

    def _prepare_register(self):
        if not self.do_not_move:
            self._set_move_files_in_hdfs()
        if self.mode == 'force' or self.mode == 'repair':
            self._set_modify_metadata('force')
        else:
            self._set_modify_metadata('insert')

    def register(self):
        if not self.do_not_move:
            self._move_files_in_hdfs()
        self._modify_metadata()
        if self.mode == 'repair':
            if self.files_delete:
                self._delete_files_in_hdfs()
                self._delete_metadata()
        logger.info('Hawq Register Succeed.')


def main(options, args):
    def connectdb(options):
        '''
        Trying to connect database, return a connection object.
        If failed to connect, raise a pg.InternalError
        '''
        url = dbconn.DbURL(hostname=options.host, port=options.port,
                           dbname=options.database, username=options.user)
        logger.info('try to connect database %s:%s %s' % (url.pghost, url.pgport, url.pgdb))
        utility_conn = pg.connect(dbname=url.pgdb, host=url.pghost, port=url.pgport,
                                  user=url.pguser, passwd=url.pgpass, opt='-c gp_session_role=utility')
        conn = pg.connect(dbname=url.pgdb, host=url.pghost, port=url.pgport,
                          user=url.pguser, passwd=url.pgpass)
        return utility_conn, conn

    # connect db
    try:
        utility_conn, conn = connectdb(options)
    except pg.InternalError:
        logger.error('Fail to connect to database, this script can only be run when database is up.')
        return 1

    failure_handler = FailureHandler(conn)
    # register
    ins = HawqRegister(options, args[0], utility_conn, conn, failure_handler)
    ins.register()
    conn.close()


if __name__ == '__main__':
    parser = option_parser()
    options, args = parser.parse_args()
    if len(args) != 1 or (options.force and options.repair):
        parser.print_help(sys.stderr)
        sys.exit(1)
    if (options.yml_config or options.force or options.repair) and options.filepath:
        parser.print_help(sys.stderr)
        sys.exit(1)
    if local_ssh('hdfs'):
        logger.error('Command "hdfs" is not available.')
        sys.exit(1)
    main(options, args)
